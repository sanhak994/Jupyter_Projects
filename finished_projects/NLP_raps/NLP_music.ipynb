{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Machine Learning to generate raps\n",
    "The goal of this project is to use a dataset with a variety of artists, filter out all the rappers, and create a corpus of their music. Then using Tensorflow I will build a neural net that will create new raps. \n",
    "\n",
    "#### Warning: Obviously a lot of rappers use explicit language. If you are not comfortable with that then do not continue\n",
    "\n",
    "Overall, the principles of this can be applied to a variety of text applications. In this case I am simply using the predict feature of my model to generate new raps, but it could also be used for missing word imputation. In otherwords, the principle can be expanded out to predictive text, where the seed is the user's sentence or word and based on a model, the next word or next full sentence is predicted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".output {\n",
       "    display: flex;\n",
       "    align-items: center;\n",
       "    text-align: center;\n",
       "    \n",
       "}\n",
       "div.output_subarea{\n",
       "    max-width:1200px;\n",
       "}\n",
       "div.text_cell_render{\n",
       "padding: 5em 5em 0.5em 0.5em\n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, HTML, Markdown, Latex\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".output {\n",
    "    display: flex;\n",
    "    align-items: center;\n",
    "    text-align: center;\n",
    "    \n",
    "}\n",
    "div.output_subarea{\n",
    "    max-width:1200px;\n",
    "}\n",
    "div.text_cell_render{\n",
    "padding: 5em 5em 0.5em 0.5em\n",
    "}\n",
    "\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"songdata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are a lot of artists, but I will broadly filter out all rappers that I know. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ABBA', 'Ace Of Base', 'Adam Sandler', 'Adele', 'Aerosmith',\n",
       "       'Air Supply', 'Aiza Seguerra', 'Alabama', 'Alan Parsons Project',\n",
       "       'Aled Jones', 'Alice Cooper', 'Alice In Chains', 'Alison Krauss',\n",
       "       'Allman Brothers Band', 'Alphaville', 'America', 'Amy Grant',\n",
       "       'Andrea Bocelli', 'Andy Williams', 'Annie', 'Ariana Grande',\n",
       "       'Ariel Rivera', 'Arlo Guthrie', 'Arrogant Worms', 'Avril Lavigne',\n",
       "       'Backstreet Boys', 'Barbie', 'Barbra Streisand', 'Beach Boys',\n",
       "       'The Beatles', 'Beautiful South', 'Beauty And The Beast',\n",
       "       'Bee Gees', 'Bette Midler', 'Bill Withers', 'Billie Holiday',\n",
       "       'Billy Joel', 'Bing Crosby', 'Black Sabbath', 'Blur', 'Bob Dylan',\n",
       "       'Bob Marley', 'Bob Rivers', 'Bob Seger', 'Bon Jovi', 'Boney M.',\n",
       "       'Bonnie Raitt', 'Bosson', 'Bread', 'Britney Spears',\n",
       "       'Bruce Springsteen', 'Bruno Mars', 'Bryan White', 'Cake',\n",
       "       'Carly Simon', 'Carol Banawa', 'Carpenters', 'Cat Stevens',\n",
       "       'Celine Dion', 'Chaka Khan', 'Cheap Trick', 'Cher', 'Chicago',\n",
       "       'Children', 'Chris Brown', 'Chris Rea', 'Christina Aguilera',\n",
       "       'Christina Perri', 'Christmas Songs', 'Christy Moore',\n",
       "       'Chuck Berry', 'Cinderella', 'Clash', 'Cliff Richard', 'Coldplay',\n",
       "       'Cole Porter', 'Conway Twitty', 'Counting Crows',\n",
       "       'Creedence Clearwater Revival', 'Crowded House', 'Culture Club',\n",
       "       'Cyndi Lauper', 'Dan Fogelberg', 'Dave Matthews Band',\n",
       "       'David Allan Coe', 'David Bowie', 'David Guetta', 'David Pomeranz',\n",
       "       'Dean Martin', 'Death', 'Deep Purple', 'Def Leppard',\n",
       "       'Demi Lovato', 'Depeche Mode', 'Devo', 'Dewa 19', 'Diana Ross',\n",
       "       'Dire Straits', 'Divine', 'Dolly Parton', 'Don Henley',\n",
       "       'Don McLean', 'Don Moen', 'Donna Summer', 'Doobie Brothers',\n",
       "       'Doors', 'Doris Day', 'Drake', 'Dream Theater',\n",
       "       'Dusty Springfield', 'Eagles', 'Ed Sheeran', 'Eddie Cochran',\n",
       "       'Electric Light Orchestra', 'Ella Fitzgerald', 'Ellie Goulding',\n",
       "       'Elton John', 'Elvis Costello', 'Elvis Presley', 'Eminem',\n",
       "       'Emmylou Harris', 'Engelbert Humperdinck', 'Enigma',\n",
       "       'Enrique Iglesias', 'Enya', 'Eppu Normaali', 'Erasure',\n",
       "       'Eric Clapton', 'Erik Santos', 'Etta James', 'Europe',\n",
       "       'Eurythmics', 'Evanescence', 'Everclear', 'Everlast', 'Exo',\n",
       "       'Exo-K', 'Extreme', 'Fabolous', 'Face To Face', 'Faces',\n",
       "       'Faith Hill', 'Faith No More', 'Falco', 'Fall Out Boy', 'Fastball',\n",
       "       'Fatboy Slim', 'Fifth Harmony', 'Fiona Apple', 'Fleetwood Mac',\n",
       "       'Flo-Rida', 'Foo Fighters', 'Foreigner', 'Frank Sinatra',\n",
       "       'Frank Zappa', 'Frankie Goes To Hollywood', 'Frankie Laine',\n",
       "       'Frankie Valli', 'Freddie Aguilar', 'Freddie King', 'Free',\n",
       "       'Freestyle', 'Fun.', 'Garth Brooks', 'Gary Numan',\n",
       "       'Gary Valenciano', 'Genesis', 'George Formby', 'George Harrison',\n",
       "       'George Jones', 'George Michael', 'George Strait', 'Gino Vannelli',\n",
       "       'Gipsy Kings', 'Glee', 'Glen Campbell', 'Gloria Estefan',\n",
       "       'Gloria Gaynor', 'GMB', 'Gordon Lightfoot', 'Grand Funk Railroad',\n",
       "       'Grateful Dead', 'Grease', 'Great Big Sea', 'Green Day',\n",
       "       'Gucci Mane', 'Guided By Voices', \"Guns N' Roses\", 'Halloween',\n",
       "       'Hank Snow', 'Hank Williams', 'Hank Williams Jr.', 'Hanson',\n",
       "       'Happy Mondays', 'Harry Belafonte', 'Harry Connick, Jr.', 'Heart',\n",
       "       'Helloween', 'High School Musical', 'Hillsong', 'Hillsong United',\n",
       "       'HIM', 'Hollies', 'Hooverphonic', 'Horrible Histories',\n",
       "       'Housemartins', 'Howard Jones', 'Human League', 'Ian Hunter',\n",
       "       'Ice Cube', 'Idina Menzel', 'Iggy Pop', 'Il Divo',\n",
       "       'Imagine Dragons', 'Imago', 'Imperials', 'Incognito', 'Incubus',\n",
       "       'Independence Day', 'Indiana Bible College', 'Indigo Girls',\n",
       "       'Ingrid Michaelson', 'Inna', 'Insane Clown Posse', 'Inside Out',\n",
       "       'INXS', 'Iron Butterfly', 'Iron Maiden', 'Irving Berlin',\n",
       "       'Isley Brothers', 'Israel', 'Israel Houghton', 'Iwan Fals',\n",
       "       'J Cole', 'Jackson Browne', 'The Jam', 'James Taylor',\n",
       "       'Janis Joplin', 'Jason Mraz', 'Jennifer Lopez', 'Jim Croce',\n",
       "       'Jimi Hendrix', 'Jimmy Buffett', 'John Denver', 'John Legend',\n",
       "       'John Martyn', 'John McDermott', 'John Mellencamp', 'John Prine',\n",
       "       'John Waite', 'Johnny Cash', 'Joni Mitchell', 'Jose Mari Chan',\n",
       "       'Josh Groban', 'Journey', 'Joy Division', 'Judas Priest', 'Judds',\n",
       "       'Judy Garland', 'Justin Bieber', 'Justin Timberlake', 'Kanye West',\n",
       "       'Kari Jobe', 'Kate Bush', 'Katy Perry', 'Keith Green',\n",
       "       'Keith Urban', 'Kelly Clarkson', 'Kelly Family', 'Kenny Chesney',\n",
       "       'Kenny Loggins', 'Kenny Rogers', 'Kid Rock', 'The Killers',\n",
       "       'Kim Wilde', 'King Crimson', 'King Diamond', 'Kinks',\n",
       "       'Kirk Franklin', 'Kirsty Maccoll', 'Kiss', 'Koes Plus', 'Korn',\n",
       "       'Kris Kristofferson', 'Kyla', 'Kylie Minogue', 'Lady Gaga',\n",
       "       'Lana Del Rey', 'Lata Mangeshkar', 'Lauryn Hill', 'Lea Salonga',\n",
       "       'Leann Rimes', 'Lenny Kravitz', 'Leo Sayer', 'Leonard Cohen',\n",
       "       'Les Miserables', 'Lil Wayne', 'Linda Ronstadt', 'Linkin Park',\n",
       "       'Lionel Richie', 'Little Mix', 'Little Walter', 'LL Cool J',\n",
       "       'Lloyd Cole', 'Lorde', 'Loretta Lynn', 'Lou Reed',\n",
       "       'Louis Armstrong', 'Louis Jordan', 'Lucky Dube', 'Luther Vandross',\n",
       "       'Lynyrd Skynyrd', 'Madonna', 'Manowar', 'Mariah Carey',\n",
       "       'Marianne Faithfull', 'Marillion', 'Marilyn Manson', 'Mark Ronson',\n",
       "       'Maroon 5', 'Mary Black', 'Matt Monro', 'Matt Redman',\n",
       "       'Mazzy Star', 'Mc Hammer', 'Meat Loaf', 'Megadeth', 'Men At Work',\n",
       "       'Metallica', 'Michael Bolton', 'Michael Buble', 'Michael Jackson',\n",
       "       'Michael W. Smith', 'Migos', 'Miley Cyrus', 'Misfits',\n",
       "       'Modern Talking', 'The Monkees', 'Moody Blues', 'Morrissey', 'Mud',\n",
       "       \"'n Sync\", 'Nat King Cole', 'Natalie Cole', 'Natalie Grant',\n",
       "       'Natalie Imbruglia', 'Nazareth', 'Ne-Yo', 'Neil Diamond',\n",
       "       'Neil Sedaka', 'Neil Young', 'New Order', 'Next To Normal',\n",
       "       'Nick Cave', 'Nick Drake', 'Nickelback', 'Nicki Minaj',\n",
       "       'Nightwish', 'Nina Simone', 'Nine Inch Nails', 'Nirvana',\n",
       "       'Nitty Gritty Dirt Band', 'Noa', 'NOFX', 'Norah Jones',\n",
       "       'Notorious B.I.G.', 'O-Zone', 'O.A.R.', 'Oasis',\n",
       "       'Ocean Colour Scene', 'Offspring', 'Ofra Haza', 'Oingo Boingo',\n",
       "       \"Old 97's\", 'Oliver', 'Olivia Newton-John', 'Olly Murs', 'Omd',\n",
       "       'One Direction', 'OneRepublic', 'Opeth', 'Orphaned Land',\n",
       "       'Oscar Hammerstein', 'Otis Redding', 'Our Lady Peace',\n",
       "       'Out Of Eden', 'Outkast', 'Overkill', 'Owl City', 'Ozzy Osbourne',\n",
       "       'Passenger', 'Pat Benatar', 'Patsy Cline', 'Patti Smith',\n",
       "       'Paul McCartney', 'Paul Simon', 'Pearl Jam', 'Perry Como',\n",
       "       'Pet Shop Boys', 'Peter Cetera', 'Peter Gabriel', 'Peter Tosh',\n",
       "       'Pharrell Williams', 'Phil Collins', 'Phineas And Ferb', 'Phish',\n",
       "       'Pink Floyd', 'Pitbull', 'Planetshakers', 'P!nk', 'Pogues',\n",
       "       'Point Of Grace', 'Poison', 'Pretenders', 'Primus', 'Prince',\n",
       "       'Proclaimers', 'Procol Harum', 'Puff Daddy', 'Q-Tip', 'Qntal',\n",
       "       'Quarashi', 'Quarterflash', 'Quasi', 'Queen', 'Queen Adreena',\n",
       "       'Queen Latifah', 'Queens Of The Stone Age', 'Queensryche',\n",
       "       'Quicksand', 'Quicksilver Messenger Service', 'Quiet Riot',\n",
       "       'Quietdrive', 'Quincy Jones', 'Quincy Punx', 'R. Kelly',\n",
       "       'Radiohead', 'Raffi', 'Rage Against The Machine', 'Rainbow',\n",
       "       'Rammstein', 'Ramones', 'Randy Travis', 'Rascal Flatts',\n",
       "       'Ray Boltz', 'Ray Charles', 'Reba Mcentire',\n",
       "       'Red Hot Chili Peppers', 'Regine Velasquez', 'Religious Music',\n",
       "       'Rem', 'Reo Speedwagon', 'Richard Marx', 'Rick Astley', 'Rihanna',\n",
       "       'Robbie Williams', 'Rod Stewart', 'Rolling Stones', 'Roxette',\n",
       "       'Roxy Music', 'Roy Orbison', 'Rush', 'Sam Smith', 'Santana',\n",
       "       'Savage Garden', 'Scorpions', 'Selah', 'Selena Gomez', 'Sia',\n",
       "       'Side A', 'Slayer', 'Smiths', 'Snoop Dogg', 'Soundgarden',\n",
       "       'Spandau Ballet', 'Squeeze', 'Starship', 'Status Quo',\n",
       "       'Steely Dan', 'Steve Miller Band', 'Stevie Ray Vaughan',\n",
       "       'Stevie Wonder', 'Sting', 'Stone Roses', 'Stone Temple Pilots',\n",
       "       'Styx', 'Sublime', 'Supertramp', 'System Of A Down',\n",
       "       'Talking Heads', 'Taylor Swift', 'Tears For Fears',\n",
       "       'The Temptations', 'Ten Years After', 'The Broadways',\n",
       "       'The Script', 'The Weeknd', 'Thin Lizzy', 'Tiffany', 'Tim Buckley',\n",
       "       'Tim McGraw', 'Tina Turner', 'Tom Jones', 'Tom Lehrer',\n",
       "       'Tom T. Hall', 'Tom Waits', 'Tool', 'Tori Amos', 'Toto',\n",
       "       'Townes Van Zandt', 'Tracy Chapman', 'Tragically Hip', 'Train',\n",
       "       'Travis', 'Twenty One Pilots', 'U. D. O.', 'U-Kiss', 'U2', 'UB40',\n",
       "       'Ufo', 'Ugly Kid Joe', \"Ultramagnetic Mc's\", 'Ultravox',\n",
       "       'Uncle Kracker', 'Uncle Tupelo', 'Underoath', 'Underworld',\n",
       "       'Unearth', 'Ungu', 'Unkle', 'Unknown', 'Unseen', 'Unwritten Law',\n",
       "       'Uriah Heep', 'Used', 'Usher', 'Utada Hikaru', 'Utopia',\n",
       "       'Van Halen', 'Van Morrison', 'Vanessa Williams', 'Vangelis',\n",
       "       'Vanilla Ice', 'Velvet Underground', 'Vengaboys', 'Venom',\n",
       "       'Vera Lynn', 'Vertical Horizon', 'Veruca Salt', 'Verve',\n",
       "       'Vince Gill', 'Violent Femmes', 'Virgin Steele', 'Vonda Shepard',\n",
       "       'Vybz Kartel', 'Walk The Moon', 'Wanda Jackson', 'Wang Chung',\n",
       "       'Warren Zevon', 'W.A.S.P.', 'Waterboys', 'Waylon Jennings', 'Ween',\n",
       "       'Weezer', 'Weird Al Yankovic', 'Westlife', 'Wet Wet Wet', 'Wham!',\n",
       "       'Whiskeytown', 'The White Stripes', 'Whitesnake',\n",
       "       'Whitney Houston', 'Who', 'Widespread Panic', 'Will Smith',\n",
       "       'Willie Nelson', 'Wilson Phillips', 'Wilson Pickett',\n",
       "       'Wishbone Ash', 'Within Temptation', 'Wiz Khalifa', 'Wu-Tang Clan',\n",
       "       'Wyclef Jean', 'X', 'X Japan', 'X-Raided', 'X-Ray Spex', 'X-Treme',\n",
       "       'Xandria', 'Xavier Naidoo', 'Xavier Rudd', 'Xentrix', 'Xiu Xiu',\n",
       "       'Xscape', 'XTC', 'Xzibit', 'Yazoo', 'Yeah Yeah Yeahs', 'Yelawolf',\n",
       "       'Yello', 'Yellowcard', 'Yeng Constantino', 'Yes', 'YG',\n",
       "       'Ying Yang Twins', 'Yngwie Malmsteen', 'Yo Gotti', 'Yo La Tengo',\n",
       "       'Yoko Ono', 'Yolanda Adams', 'Yonder Mountain String Band',\n",
       "       'You Am I', 'Young Buck', 'Young Dro', 'Young Jeezy',\n",
       "       'Youngbloodz', 'Youth Of Today', 'Yukmouth', 'Yung Joc',\n",
       "       'Yusuf Islam', 'Z-Ro', 'Zac Brown Band', 'Zakk Wylde', 'Zao',\n",
       "       'Zayn Malik', 'Zebra', 'Zebrahead', 'Zed', 'Zero 7', 'Zeromancer',\n",
       "       'Ziggy Marley', 'Zoe', 'Zoegirl', 'Zornik', 'Zox', 'Zucchero',\n",
       "       'Zwan', 'ZZ Top', 'Joseph And The Amazing Technicolor Dreamcoat',\n",
       "       'Soundtracks', 'Van Der Graaf Generator', 'Various Artists',\n",
       "       'Zazie'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['artist'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = [\n",
    "     'Drake', 'Eminem', 'Fabolous', \n",
    "       'Fatboy Slim', \n",
    "       'Flo-Rida',\n",
    "       'Gucci Mane', \n",
    "       'Ice Cube',\n",
    "       'J Cole',  'Kanye West', 'Lil Wayne', 'LL Cool J','Mc Hammer', \n",
    "        'Migos', 'Nicki Minaj',\n",
    "       'Notorious B.I.G.', 'Puff Daddy', 'Q-Tip', 'Snoop Dogg', \n",
    "       'The Weeknd','Will Smith',\n",
    "        'Wiz Khalifa', 'Wu-Tang Clan', 'Yelawolf',\n",
    "       'Ying Yang Twins', 'Yo Gotti', 'Young Buck', 'Young Dro', 'Young Jeezy',\n",
    "       'Youngbloodz', 'Yung Joc'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rap = df[df['artist'].isin(artists)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sanjeed/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_rap['text'].to_csv(r'df_rap.txt', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sanjeed/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['text'].to_csv(r'df_all.txt', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"df_rap.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(path_to_file, 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is part of the finished corpus that will go into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"[Verse 1]  \n",
      "Boomin' out in South Gwinnett like Lou Will  \n",
      "6 man like Lou Will, 2 girls and they get along like I'm...  \n",
      "Like I'm Lou Will, I just got the new deal  \n",
      "I am in the Matrix and I just took the blue pill  \n",
      "No ho shit, no fuckin' ho shit, just save that for your shit  \n",
      "I don't need no fuckin' body, I run my own shit  \n",
      "Like Soulja, I thought I told yah, you didn't listen  \n",
      "Fieri, I'm in the kitchen, I'm a magician  \n",
      "I'm on it, I'm like Macgyver, I'm Michael Meyers  \n",
      "I kill careers and c\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 76 unique vocab words in the entire rap corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model needs every vocab item to be assigned numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ind = {char:ind for ind,char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " \"'\": 4,\n",
       " '(': 5,\n",
       " ')': 6,\n",
       " ',': 7,\n",
       " '-': 8,\n",
       " '.': 9,\n",
       " '0': 10,\n",
       " '1': 11,\n",
       " '2': 12,\n",
       " '3': 13,\n",
       " '4': 14,\n",
       " '5': 15,\n",
       " '6': 16,\n",
       " '7': 17,\n",
       " '8': 18,\n",
       " '9': 19,\n",
       " ':': 20,\n",
       " '?': 21,\n",
       " 'A': 22,\n",
       " 'B': 23,\n",
       " 'C': 24,\n",
       " 'D': 25,\n",
       " 'E': 26,\n",
       " 'F': 27,\n",
       " 'G': 28,\n",
       " 'H': 29,\n",
       " 'I': 30,\n",
       " 'J': 31,\n",
       " 'K': 32,\n",
       " 'L': 33,\n",
       " 'M': 34,\n",
       " 'N': 35,\n",
       " 'O': 36,\n",
       " 'P': 37,\n",
       " 'Q': 38,\n",
       " 'R': 39,\n",
       " 'S': 40,\n",
       " 'T': 41,\n",
       " 'U': 42,\n",
       " 'V': 43,\n",
       " 'W': 44,\n",
       " 'X': 45,\n",
       " 'Y': 46,\n",
       " 'Z': 47,\n",
       " '[': 48,\n",
       " ']': 49,\n",
       " 'a': 50,\n",
       " 'b': 51,\n",
       " 'c': 52,\n",
       " 'd': 53,\n",
       " 'e': 54,\n",
       " 'f': 55,\n",
       " 'g': 56,\n",
       " 'h': 57,\n",
       " 'i': 58,\n",
       " 'j': 59,\n",
       " 'k': 60,\n",
       " 'l': 61,\n",
       " 'm': 62,\n",
       " 'n': 63,\n",
       " 'o': 64,\n",
       " 'p': 65,\n",
       " 'q': 66,\n",
       " 'r': 67,\n",
       " 's': 68,\n",
       " 't': 69,\n",
       " 'u': 70,\n",
       " 'v': 71,\n",
       " 'w': 72,\n",
       " 'x': 73,\n",
       " 'y': 74,\n",
       " 'z': 75}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_to_char = np.array(vocab) ## index loc to grab char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([char_to_ind[c] for c in text]) ## corpus as np.array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching: Sequence length must be long enough to capture the structure. I think a length of about 180 is a good starting place. Since most raps are couplets, I will also shorten that to 80 in another model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n the low  \n",
      "I still been plottin' on the low  \n",
      "Schemin' on the low  \n",
      "The furthest thing from perfect  \n",
      "Like everyone I know  \n",
      "  \n",
      "I just been drinkin' on the low  \n",
      "Mobbin' on the low  \n",
      "Fuckin' on the low  \n",
      "Smokin' on the low  \n",
      "I just been plottin' on the low  \n",
      "Schemin' on the low  \n",
      "The furthest thing from perfect  \n",
      "Like everyone I know  \n",
      "  \n",
      "Drinkin', smokin', fuckin', plottin'  \n",
      "Schemin', plottin', schemin', gettin' money  \n",
      "Drinkin', fuckin', smokin', plottin', schemin'  \n",
      "Plottin', schemin', getting money  \n",
      "  \n",
      "This the life for me  \n",
      "My mama told me this was right for me  \n",
      "I got em worried, like make sure you save a slice for me  \n",
      "I should have Spoons, serve you up with a fork and knife for me  \n",
      "Your actions make us doubt you  \n",
      "Your lack of effort got me rapping different  \n",
      "This the shit I wanna go out to  \n",
      "Play this shit at my funeral if they catch me slippin'  \n",
      "Naked women swimming that's just how I'm living  \n",
      "Donate a million to some children, that's just how I'm feeling  \n",
      "A nigga fil\n"
     ]
    }
   ],
   "source": [
    "print(text[18000:19000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len('''\n",
    "# Trees like me weren't meant to live  \n",
    "# (Oh Lord I lay me down)  \n",
    "# If all this earth can give  \n",
    "# (My branches to the ground)  \n",
    "# Is pollution and slow death  \n",
    "# (There's nothing left for me) \n",
    "# ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 23145 sequences based on the batch length in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23145"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_seq = len(text) // seq_len+1\n",
    "total_num_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_len+1, drop_remainder=True) ## drop incomplete seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Need a way to loop and append every character\n",
    "def create_seq_target(seq):\n",
    "    input_txt = seq[:-1] # Hello my nam\n",
    "    target_txt = seq[1:] # ello my name\n",
    "    return input_txt, target_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(create_seq_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buffer: I don't want to load everything into memory at once, so I will only grab about 10k sequences at a time and shuffle them around to make the model robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 10000\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 64 ## keep around unique number of vocab elems\n",
    "rnn_neurons = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data is not actually on-hot-encoded so I have to use this func when fitting\n",
    "def sparse_cat_loss(y_true, y_pred):\n",
    "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_dim, batch_input_shape=[batch_size, None]))\n",
    "    model.add(GRU(rnn_neurons, return_sequences=True,\n",
    "                 stateful=True, recurrent_initializer='glorot_uniform'))\n",
    "    model.add(Dense(vocab_size))\n",
    "    \n",
    "    model.compile('adam', loss=sparse_cat_loss)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (128, None, 64)           4864      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (128, None, 1024)         3348480   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (128, None, 76)           77900     \n",
      "=================================================================\n",
      "Total params: 3,431,244\n",
      "Trainable params: 3,431,244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size=vocab_size,\n",
    "                    embed_dim=embed_dim,\n",
    "                    rnn_neurons=rnn_neurons,\n",
    "                    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 30\n",
    "# model.fit(dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation: Given the number of parameters, I had to use a GPU on Google Colab to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 64)             4864      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3348480   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 76)             77900     \n",
      "=================================================================\n",
      "Total params: 3,431,244\n",
      "Trainable params: 3,431,244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('NLP_songs_fixed.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_seed, gen_size=500, temp=1.0):\n",
    "    num_generate= gen_size\n",
    "    \n",
    "    input_eval = [char_to_ind[s] for s in start_seed]\n",
    "    \n",
    "    input_eval = tf.expand_dims(input_eval,0)\n",
    "    \n",
    "    text_generated = []\n",
    "    \n",
    "    temperature = temp\n",
    "    \n",
    "    model.reset_states()\n",
    "    \n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0) ## undoes the expanded dims above\n",
    "        predictions = predictions / temperature ## affect actual prob of dist. based on \n",
    "        ## what we set temp as\n",
    "        \n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        input_eval = tf.expand_dims([predicted_id],0)\n",
    "        \n",
    "        text_generated.append(ind_to_char[predicted_id])\n",
    "        \n",
    "    return (start_seed + \"\".join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the first rap from model 1, which has a sequence length of 180. The model predicts everything after the start seed. \n",
    "The made up words are to be expected as it is predicting a character at a time. \n",
    "\n",
    "I also made two other models after this one. I think the results from the second model are the best. Clearly, some of the sentences are nonsense, however, the results are still quite impressive. Even on a Google Colab Nvidia GPU, these models took a bit of time to run. So, in the future I would like to replicate this project with better hardware that could allow me to run for far more epochs with several layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (1, None, 64)             4864      \n",
      "_________________________________________________________________\n",
      "gru_10 (GRU)                 (1, None, 1024)           3348480   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (1, None, 76)             77900     \n",
      "=================================================================\n",
      "Total params: 3,431,244\n",
      "Trainable params: 3,431,244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\n",
    "model.load_weights('NLP_songs_fixed.h5')\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The start seed is 'yo '\n",
      "\n",
      "yo Good but why shoulda say hah hah!  \n",
      "(Don't get it cause there's right)  \n",
      "We gettin' chedical endy or the 40's  \n",
      "The Loudest place that we talkin' to myself  \n",
      "I feel I'm bounce you know my money  \n",
      "Muthafuckas went to H4M  \n",
      "King color groupies on the pool conversation  \n",
      "We back to the bomb, buddy body and  \n",
      "Back like a splinter, want you to see if he continued the kid  \n",
      "Hard to take the shit is where you're doing too much, but it best white,  \n",
      "Soon tell the pretty get tolike stealing my face again\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    ################################################\n",
      "    ################################################\n",
      "    ################################################\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "The start seed is 'shawty'\n",
      "\n",
      "shawty  \n",
      "Caugar to hit me out like rain  \n",
      "Bitches he act like it's slow and ballers  \n",
      "That's why I be the pain just to get a good for me je  \n",
      "Everybody tryin' to hear the same ring the alley  \n",
      "Shit go for forgiveness, so clean up hard as hell  \n",
      "Watchinaters in the club that's my coantry  \n",
      "And while the weed cold in a bow  \n",
      "U-Aw I ran hardcore his drink fighter  \n",
      "Sux my isl's bout six of her languap  \n",
      "Way I think I need to be right  \n",
      "Now Cole World True-work gotta buck wild  \n",
      "Somebody help me, a baby  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    ################################################\n",
      "    ################################################\n",
      "    ################################################\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "The start seed is 'hommie'\n",
      "\n",
      "hommies decision carass your spot (Gucci)  \n",
      "  \n",
      "Not but the weed in my facing wit me  \n",
      "Codeine salt left that's mark like the trouble raised this nigga feelin' like a Capitor There we pack and breathing, let's take it  \n",
      "Gettin' my car worst first step, I ain't black lean  \n",
      "A freak'll bad baby momma brother, a tale rate  \n",
      "Two record but my shit is with hustlers and shituff  \n",
      "He get suffend the days of my friend  \n",
      "Look at the store for me cause he'll be still drinkin' on me  \n",
      "Don't wanna be the only ones\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    ################################################\n",
      "    ################################################\n",
      "    ################################################\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "The start seed is 'rachet'\n",
      "\n",
      "rachet dollar buddy  \n",
      "Drippin' bout's good to meet Kiz K for shobin with our lives  \n",
      "I do it for the gyeat  \n",
      "It's probably wouldn't be in there  \n",
      "Looking for the ambulance and money friends  \n",
      "Long knows you wrote you with a slug  \n",
      "I think i'll bring your spot  \n",
      "Say that the O just think if I know is you might as well retrey me  \n",
      "  \n",
      "Come on, come on, come on, come on  \n",
      "Shaolin hall, like that  \n",
      "It's around the woods I have to\n",
      "  \n",
      "Buk-beastring, vook over in your heart no reason  \n",
      "No more like a nigga\n",
      "pl\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    ################################################\n",
      "    ################################################\n",
      "    ################################################\n",
      "    \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "terms = ['yo ', 'shawty', 'hommie', 'rachet']\n",
    "\n",
    "for item in terms:\n",
    "    \n",
    "    print(f\"The start seed is '{item}'\\n\")\n",
    "    print(generate_text(model, \n",
    "                    start_seed=item,\n",
    "                   gen_size=500,\n",
    "                   temp=1.0))\n",
    "    print('\\n\\n')\n",
    "    print('''\n",
    "    ################################################\n",
    "    ################################################\n",
    "    ################################################\n",
    "    ''')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: sequence was reduced to 80 and the number of embed_dims was reduced to 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (1, None, 50)             3800      \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (1, None, 1024)           3305472   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (1, None, 76)             77900     \n",
      "=================================================================\n",
      "Total params: 3,387,172\n",
      "Trainable params: 3,387,172\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size, 50, rnn_neurons, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('NLP_songs_fixed2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The start seed is 'yo '\n",
      "\n",
      "yo people as to four I'll be my friend a bad, my money getcha  \n",
      "I said, I'm gone off that pound of dank (that sticky, that icky) \n",
      "Richand trying to plenty to be the dirty Charast (se if you wanna get right  \n",
      "Let's get right  \n",
      "Let's get right to A bitch she step  \n",
      "Now how the fuck would go for me  \n",
      "Even though ya rut thout five to my legs  \n",
      "What's your sugher fat lil shit that you see it?s all the fingers?  \n",
      "Could be back for the future [?] Plue Dogg I just outld A-N, D, Robay Rover  \n",
      "Bustin' of the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    ################################################\n",
      "    ################################################\n",
      "    ################################################\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "The start seed is 'shawty'\n",
      "\n",
      "shawty got a 9Uhow  \n",
      "I see my only one was gone, thou shoo shook up  \n",
      "And all you need is shady thin swent, show her out  \n",
      "  \n",
      "Got plenty for a while far  for none shit  \n",
      "She say she sweet budging they had a double really cared  \n",
      "Mcs Deville,  \n",
      "Cause that you've always been a thug like in side, shootin bottles off throwed a Jazz Cash Money's cub  \n",
      "Shats worth a mollye bag and you got crazy dope nigga, put it back to the blocks happened by a rapper  \n",
      "  \n",
      "To the sky to his n'all so not in the cover of my \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    ################################################\n",
      "    ################################################\n",
      "    ################################################\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "The start seed is 'hommie'\n",
      "\n",
      "hommies flowed  \n",
      "Look good with these hown' live role money  \n",
      "Don't take hella  \n",
      "I want to be  \n",
      "The only twin you was meant  \n",
      "I'm in my veins, knowncathing like real  \n",
      "Talk baby, see that they stay in the fucking life\n",
      "\n",
      "\"\n",
      "\"I had to ride  \n",
      "for all dem a gwaan  \n",
      "That's a blew if you just know where we don't want me to love me though  \n",
      "Get read, for twenty for life  \n",
      "Oh yeah Smooth control  \n",
      "If you (damn) to the dance moiline]  \n",
      "Let's put the act like a solution  \n",
      "Man style while you got it  \n",
      "And everybod\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    ################################################\n",
      "    ################################################\n",
      "    ################################################\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "The start seed is 'rachet'\n",
      "\n",
      "rachet  \n",
      "(Haha I live guide you niggas is no college in this club jumps  \n",
      "Trying to get at the pilot  \n",
      "Game up an UPtir to play me crazy dollars do  \n",
      "The sh-t it, you jump right up to that money name (halk)  \n",
      "Go ahead, ta the dun' you  \n",
      "Show him wols with the wife  \n",
      "The way faster and then I made it the aftern-thy  \n",
      "The hustle man with no you icollabogng  \n",
      "So what can your mouth  \n",
      "It ain't abuition yet you got me being us.Butiness\n",
      "\"\"Oh baby, for heavens sake\"\"\n",
      "\n",
      "\"\n",
      "\"[Intro]  \n",
      "Yep yep, I drimm girl  \n",
      "Cau\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    ################################################\n",
      "    ################################################\n",
      "    ################################################\n",
      "    \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "terms = ['yo ', 'shawty', 'hommie', 'rachet']\n",
    "\n",
    "for item in terms:\n",
    "    \n",
    "    print(f\"The start seed is '{item}'\\n\")\n",
    "    print(generate_text(model, \n",
    "                    start_seed=item,\n",
    "                   gen_size=500,\n",
    "                   temp=1.0))\n",
    "    print('\\n\\n')\n",
    "    print('''\n",
    "    ################################################\n",
    "    ################################################\n",
    "    ################################################\n",
    "    ''')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Increased neurons to 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (1, None, 50)             3800      \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (1, None, 2048)           12902400  \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (1, None, 76)             155724    \n",
      "=================================================================\n",
      "Total params: 13,061,924\n",
      "Trainable params: 13,061,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size, 50, 2048 , batch_size=1)\n",
    "model.load_weights('NLP_songs_fixed3.h5')\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The start seed is 'yo '\n",
      "\n",
      "yo  \n",
      "Ain't no way on her ass roader than that part from hom and oh party again ax we just hold accord!  \n",
      "Ya'll wanna be argelard your dudest eat  \n",
      "'cause she mean not, murda-be ness (All my  is? You d \n",
      "Fuck them  \n",
      "Get lifting  \n",
      "I don't speak to me  \n",
      "What you can't skept feel  \n",
      "Cause your lyin' leave  \n",
      "Come with my mental ou know it's a nine or the old my night  \n",
      "Rock-and who that little libbatch the Couse and this sumier contact  \n",
      "And I don't wanna fin'  \n",
      "I want that I be sick  \n",
      "But the same one of years, we felt russy literal shit, pull it up, rollin' it up  \n",
      "  \n",
      "Your nasty got a nigga  \n",
      "But but I was roight to hold my stale  \n",
      "You know what your striend and com  \n",
      "Turn in Snips with thave,  \n",
      "I said, I'm sincy  \n",
      "No yatrilocause to drop tough with fat five hard  \n",
      "Felons it was youngin'  \n",
      "You too\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    ################################################\n",
      "    ################################################\n",
      "    ################################################\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "The start seed is 'shawty'\n",
      "\n",
      "shawty, stop left, looking all rem brugged, know my feels get talk to the AKL-Son (cham)  \n",
      "My paring other money the front on a real man  \n",
      "A stop on the Rap  \n",
      "Pastor Take anybody all takes, and get scared off time 'em all not that I'm not they want it  \n",
      "Sneak like that's when I did it  \n",
      "Feel game\"\"  \n",
      "  \n",
      "Survive and she won't shop  \n",
      "Uh baby you show a pacable  \n",
      "Come and all change  \n",
      "Don't know what to deal with swials  \n",
      "you, and move tonight  \n",
      "Let me beat a kiss  \n",
      "I'm a burlaund  \n",
      "We concerned that they have one all the way to Get her ton't shock  \n",
      "When I wake up  \n",
      "Baby, damn I'm finishat they mind  \n",
      "And if you ain't say now  \n",
      "So slow move his memory  \n",
      "the coke up some paper (Hatha!)  \n",
      "Frout night like you ready to miss ya  \n",
      "I still the harder got my green  \n",
      "Young Dro  \n",
      "Shett.... ggas  \n",
      "Getting h\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    ################################################\n",
      "    ################################################\n",
      "    ################################################\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "The start seed is 'hommie'\n",
      "\n",
      "hommie on, broaght at the people  \n",
      "Ly irrealin'  \n",
      "  \n",
      "And a 100 oc cherts feel it don't  \n",
      "I might put ut niggas gun freakin' down Gill Gpittingbors at the buttas in it  \n",
      "Wait from the mornin'  \n",
      "Gotta  skepion, I won't nece be a bitch  \n",
      "But I'm so iced out  \n",
      "So don't no sitular...  \n",
      "See it's stop t I got look  \n",
      "Col its then I put that pussy one whole passine  \n",
      "Trying to get along  \n",
      "And my mother so f the dope boy!  \n",
      "(A-Yo-G) il up the radio?  \n",
      "Oh while i told my, \"\"Fick way before miss is talkin around you?  \n",
      "You can see me all take trip, so I can up ychomus to your name  \n",
      "  \n",
      "[Ohile the prewist  \n",
      "Her love the cash, yeah we talkin to the private sknow  \n",
      "Don't about to make me growin  \n",
      "Man, from just p with a sack  \n",
      "  \n",
      "[Chorus]  \n",
      "  \n",
      "I niggaz is socibe  \n",
      "Bitch some shit  \n",
      "You know, th, I'm lying til \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    ################################################\n",
      "    ################################################\n",
      "    ################################################\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "The start seed is 'rachet'\n",
      "\n",
      "rachet  \n",
      "Or the club told me spendin tind with a baibfir bitch, now pack tha fool  \n",
      "See, we the homies lonely wribble  \n",
      "And what I'm sayin', then hunt  \n",
      "Now te run where we do the back  \n",
      "Ouh the way I proboby Poldin't lime story lookin for what you goin up all goon  \n",
      "Niggas is cakin to walk you, nowe g to be in lust to my tendin'  \n",
      "Business is showin up out the matchin' and still roped in them  \n",
      "turn to niggaz do trippin' like a sunshalet in them dumb niggas  \n",
      "Say \"\"No\"\"  \n",
      "Stip tight hurted with 'em  \n",
      "I think ya fuck wit 'em hopts 'em with me, like later  \n",
      "But its bareah doing whed even our cause you probably be care get to ite  \n",
      "Then my baby fo' gives a tube short, gotta gotta ship  \n",
      "She be tryin' to hol guys list cause in the mix up hach like \"\"Hey-hey yo, Gosta baby girl, I'm gon' betty you l\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    ################################################\n",
      "    ################################################\n",
      "    ################################################\n",
      "    \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "terms = ['yo ', 'shawty', 'hommie', 'rachet']\n",
    "\n",
    "for item in terms:\n",
    "    \n",
    "    print(f\"The start seed is '{item}'\\n\")\n",
    "    print(generate_text(model, \n",
    "                    start_seed=item,\n",
    "                   gen_size=800,\n",
    "                   temp=1.0))\n",
    "    print('\\n\\n')\n",
    "    print('''\n",
    "    ################################################\n",
    "    ################################################\n",
    "    ################################################\n",
    "    ''')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
